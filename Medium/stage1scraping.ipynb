{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charity Scraper\n",
    "### The Plan:\n",
    "1. Stage 1 scraping: Scrape Each Charity's register_index URL from the main search page\n",
    "1. Stage 2 scraping: Open each charity and scrape characteristics\n",
    "1. Stage 3 analysis:\n",
    "  - clean date data\n",
    "  - Use generated charity_index to filter for websites with desired characteristics\n",
    "    * income\n",
    "    * reporting\n",
    "    * cause helped\n",
    "    * has website and charity_url\n",
    "\n",
    "## Stage 1 Scraping\n",
    "#### todo:\n",
    "- scrape 10 at a time\n",
    "- rerun\n",
    "- add placeholder headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation and loadup\n",
    "from http import server\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import selenium as sl\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driverpath=ChromeDriverManager().install()\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "size = 'Medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the pandas dataframe\n",
    "columns = [\"Index_Link\", \"Legal_Name\", \"Town/Suburb\",\"State\", \"Status\", \"Size\",\"ABN\",\"Website\",\"Revenue\",\"Expenses\",\"Cause/s\",\"Last_report_date\",\"ref_religion\"]\n",
    "try:\n",
    "    os.mkdir('s1scrapedata')\n",
    "except:\n",
    "    None\n",
    "\n",
    "try:\n",
    "    os.mkdir('s2scrapedata')\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getpageurl(p):\n",
    "    lefturl=\"https://www.acnc.gov.au/charity/charities?items_per_page=100&page=\"\n",
    "    pagenumber=str(p)\n",
    "    righturl=\"&f[]=size%3A\"+size\n",
    "    \n",
    "    url = lefturl+pagenumber+righturl #pre filtered url for small businesses and to show 100 results - Page 1\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_data(d):\n",
    "    try:\n",
    "        WebDriverWait(d,90).until(EC.presence_of_element_located((By.TAG_NAME,\"tbody\")))\n",
    "        time.sleep(1)\n",
    "        table=d.find_element(by=By.TAG_NAME, value=\"tbody\")\n",
    "        rows=table.find_elements(by=By.TAG_NAME,value=\"tr\")\n",
    "        \n",
    "        for row in rows:\n",
    "            link   = row.find_element(by=By.TAG_NAME,value=\"a\").get_attribute(\"href\")\n",
    "            name   = row.find_elements(by=By.TAG_NAME, value=\"td\")[0].text\n",
    "            town   = row.find_elements(by=By.TAG_NAME, value=\"td\")[1].text\n",
    "            state  = row.find_elements(by=By.TAG_NAME, value=\"td\")[2].text\n",
    "            status = row.find_elements(by=By.TAG_NAME, value=\"td\")[3].text\n",
    "            size   = row.find_elements(by=By.TAG_NAME, value=\"td\")[4].text\n",
    "            abn    = row.find_elements(by=By.TAG_NAME, value=\"td\")[5].text\n",
    "\n",
    "            row_data = pd.DataFrame([link,name,town,state,status,size,abn,None,None,None,None,None,None], index=columns).transpose()\n",
    "            try:\n",
    "                page_data = pd.concat([page_data,row_data],ignore_index=True)\n",
    "            except:\n",
    "                page_data = row_data\n",
    "        \n",
    "        return page_data\n",
    "    except:\n",
    "        print('Table could not be found on page'+str(page))\n",
    "        return None \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   df=pd.DataFrame([0,1,'b'], index=[10,20,30]).transpose()\n",
    "#   b=pd.DataFrame([2,3,5], index=[10,20,30]).transpose()\n",
    "#   df=pd.concat([df,b],ignore_index=True)\n",
    "#   print(df)\n",
    "#   df=pd.concat([df,copy.deepcopy(b)],ignore_index=True)\n",
    "#   print(df)\n",
    "#   \n",
    "#   df.to_csv('./folder/file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]\n",
      "[[0 0 0]\n",
      " [1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([0,0,0])\n",
    "print(a)\n",
    "a=np.vstack((a,[1,2,3]))\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_drivers(num_drivers=10):\n",
    "    drivers =[]\n",
    "    for i in range(num_drivers):\n",
    "        drivers.append(webdriver.Chrome(service=Service(driverpath)))\n",
    "    return drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scraper():\n",
    "    drivers = open_drivers(3)\n",
    "    try:\n",
    "        #navigate into pages and scrape\n",
    "        page=0\n",
    "        totalpages= 83 - 1 #actual pages-1\n",
    "        try:\n",
    "            ScrapedSearchPages = np.load('./s1scrapedata/scrapedpageindex.npy')\n",
    "        except:\n",
    "            ScrapedSearchPages = np.array([])\n",
    "\n",
    "        tempdf = pd.DataFrame()\n",
    "\n",
    "        while page < totalpages:\n",
    "            #load up sites\n",
    "            pagecopy = copy.deepcopy(page)\n",
    "            for d in drivers:\n",
    "                while pagecopy in ScrapedSearchPages:\n",
    "                    pagecopy+=1\n",
    "                if pagecopy < totalpages:\n",
    "                    d.get(getpageurl(pagecopy))        \n",
    "                    pagecopy+=1\n",
    "\n",
    "            ## scraping\n",
    "            for D in drivers:\n",
    "                while page in ScrapedSearchPages:\n",
    "                    page +=1\n",
    "                page_data = extract_page_data(d=D)\n",
    "                tempdf=pd.concat((tempdf,page_data),ignore_index=True)\n",
    "                ScrapedSearchPages=np.append(ScrapedSearchPages, page)\n",
    "\n",
    "                if (page+1) % 15 == 0:\n",
    "                    np.save('./s1scrapedata/scrapedpageindex.npy',ScrapedSearchPages)\n",
    "                    tempdf.to_csv('./s1scrapedata/S1scrape_topage'+str(page+1)+'.csv')\n",
    "                    tempdf = pd.DataFrame()\n",
    "    \n",
    "                page += 1\n",
    "                print(\"Progress: \"+str(page)+\"/\"+str(totalpages+1))\n",
    "\n",
    "        #final Save\n",
    "        np.save('./s1scrapedata/scrapedpageindex.npy',ScrapedSearchPages)\n",
    "        tempdf.to_csv('./s1scrapedata/S1scrape_topage'+str(page+1)+'.csv')\n",
    "        tempdf = pd.DataFrame()\n",
    "\n",
    "\n",
    "        #kill drivers\n",
    "        for d in drivers:\n",
    "            d.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        for d in drivers:\n",
    "            d.quit()\n",
    "        raise e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 31/83\n",
      "Progress: 32/83\n",
      "Progress: 33/83\n",
      "Progress: 34/83\n",
      "Progress: 35/83\n",
      "Progress: 36/83\n",
      "Progress: 37/83\n",
      "Progress: 38/83\n",
      "Progress: 39/83\n",
      "Progress: 40/83\n",
      "Progress: 41/83\n",
      "Progress: 42/83\n",
      "Progress: 43/83\n",
      "Progress: 44/83\n",
      "Progress: 45/83\n",
      "Progress: 46/83\n",
      "Progress: 47/83\n",
      "Progress: 48/83\n",
      "Progress: 49/83\n",
      "Progress: 50/83\n",
      "Progress: 51/83\n",
      "Progress: 52/83\n",
      "Progress: 53/83\n",
      "Progress: 54/83\n",
      "Progress: 55/83\n",
      "Progress: 56/83\n",
      "Progress: 57/83\n",
      "Progress: 58/83\n",
      "Progress: 59/83\n",
      "Progress: 60/83\n",
      "Progress: 61/83\n",
      "Progress: 62/83\n",
      "Progress: 63/83\n",
      "Progress: 64/83\n",
      "Progress: 65/83\n",
      "Progress: 66/83\n",
      "Progress: 67/83\n",
      "Progress: 68/83\n",
      "Progress: 69/83\n",
      "Progress: 70/83\n",
      "Progress: 71/83\n",
      "Progress: 72/83\n",
      "Progress: 73/83\n",
      "Progress: 74/83\n",
      "Progress: 75/83\n",
      "Progress: 76/83\n",
      "Progress: 77/83\n",
      "Progress: 78/83\n",
      "Progress: 79/83\n",
      "Progress: 80/83\n",
      "Progress: 81/83\n",
      "Progress: 82/83\n",
      "Progress: 83/83\n",
      "Progress: 84/83\n"
     ]
    }
   ],
   "source": [
    "run_scraper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
